{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv data_ingestion/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sshtunnel import SSHTunnelForwarder\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pgvector\n",
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "import json\n",
    "def get_db_connection():\n",
    "    # Setting up the SSH tunnel with tunnel credentials\n",
    "    REMOTE_HOST = os.getenv(\"REMOTE_HOST\")\n",
    "    REMOTE_SSH_PORT = int(os.getenv(\"REMOTE_SSH_PORT\"))\n",
    "    PORT = int(os.getenv(\"PORT\"))\n",
    "    SSH_KEYFILE = os.getenv(\"SSH_KEYFILE\")\n",
    "    SSH_USERNAME =  os.getenv(\"SSH_USERNAME\")\n",
    "\n",
    "    server = SSHTunnelForwarder(\n",
    "        ssh_address_or_host=(REMOTE_HOST, REMOTE_SSH_PORT),\n",
    "        ssh_username= SSH_USERNAME,\n",
    "        ssh_pkey=SSH_KEYFILE,\n",
    "        # Key part! Connect to AWS_HOST through the tunnel.\n",
    "        remote_bind_address=('localhost', PORT)\n",
    "    )\n",
    "    server.start()\n",
    "    print(\"server connected\")\n",
    "\n",
    "    conn_str = f\"dbname=postgres host=localhost port={server.local_bind_port} user=postgres password={os.getenv('DB_PASSWORD')}\"\n",
    "    conn_str_formatted = f\"postgresql://postgres:{os.getenv('DB_PASSWORD')}@localhost:{server.local_bind_port}/postgres\"\n",
    "    return conn_str_formatted, conn_str, psycopg.connect(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.35-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.10.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_core-0.3.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.123-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.11.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/abeomorogbe/Library/Python/3.11/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/abeomorogbe/Library/Python/3.11/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.7-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading anyio-4.5.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sniffio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.5-cp311-cp311-macosx_11_0_arm64.whl (388 kB)\n",
      "Downloading langchain_core-0.3.1-py3-none-any.whl (405 kB)\n",
      "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.123-py3-none-any.whl (290 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Downloading SQLAlchemy-2.0.35-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading orjson-3.10.7-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (251 kB)\n",
      "Downloading yarl-1.11.1-cp311-cp311-macosx_11_0_arm64.whl (112 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyio-4.5.0-py3-none-any.whl (89 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: tenacity, SQLAlchemy, sniffio, PyYAML, pydantic-core, orjson, numpy, multidict, jsonpointer, h11, frozenlist, attrs, annotated-types, aiohappyeyeballs, yarl, pydantic, jsonpatch, httpcore, anyio, aiosignal, httpx, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.1\n",
      "    Uninstalling numpy-2.1.1:\n",
      "      Successfully uninstalled numpy-2.1.1\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.5.0 attrs-24.2.0 frozenlist-1.4.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-core-0.3.1 langchain-text-splitters-0.3.0 langsmith-0.1.123 multidict-6.1.0 numpy-1.26.4 orjson-3.10.7 pydantic-2.9.2 pydantic-core-2.23.4 sniffio-1.3.1 tenacity-8.5.0 yarl-1.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY= os.getenv(\"AZURE_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", \n",
    "                                   api_key=AZURE_OPENAI_API_KEY, \n",
    "                                   azure_endpoint=AZURE_OPENAI_ENDPOINT, \n",
    "                                   azure_deployment=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is the framework for building context-aware reasoning applications'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text],\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "# show the retrieved document's content\n",
    "retrieved_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Downloading psycopg2_binary-2.9.9-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pgvector in /opt/homebrew/lib/python3.11/site-packages (0.2.5)\n",
      "Collecting pgvector\n",
      "  Using cached pgvector-0.3.3-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from pgvector) (1.26.4)\n",
      "Using cached pgvector-0.3.3-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pgvector\n",
      "  Attempting uninstall: pgvector\n",
      "    Found existing installation: pgvector 0.2.5\n",
      "    Uninstalling pgvector-0.2.5:\n",
      "      Successfully uninstalled pgvector-0.2.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-postgres 0.0.12 requires pgvector<0.3.0,>=0.2.5, but you have pgvector 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pgvector-0.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server connected\n",
      "postgresql://postgres:Caroot-gen-0@localhost:58219/postgres\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "conn = get_db_connection()\n",
    "collection_name = \"lc_playground\"\n",
    "#connection=\"postgresql+psycopg://\"+ get_db_connection()\n",
    "print(conn)\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=conn,\n",
    "    use_jsonb=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"there are cats in the pond\",\n",
    "        metadata={\"id\": 1, \"location\": \"pond\", \"topic\": \"animals\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ducks are also found in the pond\",\n",
    "        metadata={\"id\": 2, \"location\": \"pond\", \"topic\": \"animals\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"fresh apples are available at the market\",\n",
    "        metadata={\"id\": 3, \"location\": \"market\", \"topic\": \"food\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the market also sells fresh oranges\",\n",
    "        metadata={\"id\": 4, \"location\": \"market\", \"topic\": \"food\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the new art exhibit is fascinating\",\n",
    "        metadata={\"id\": 5, \"location\": \"museum\", \"topic\": \"art\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"a sculpture exhibit is also at the museum\",\n",
    "        metadata={\"id\": 6, \"location\": \"museum\", \"topic\": \"art\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"a new coffee shop opened on Main Street\",\n",
    "        metadata={\"id\": 7, \"location\": \"Main Street\", \"topic\": \"food\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the book club meets at the library\",\n",
    "        metadata={\"id\": 8, \"location\": \"library\", \"topic\": \"reading\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the library hosts a weekly story time for kids\",\n",
    "        metadata={\"id\": 9, \"location\": \"library\", \"topic\": \"reading\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"a cooking class for beginners is offered at the community center\",\n",
    "        metadata={\"id\": 10, \"location\": \"community center\", \"topic\": \"classes\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "vector_store.add_documents(docs, ids=[doc.metadata[\"id\"] for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_embeddings(texts,embeddings,metadatas,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_embedding_pairs = list(zip(texts, text_embeddings))\n",
    "vectorstore = PGVector.from_embeddings(text_embedding_pairs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the VectorSearchChain\n",
    "vector_search_chain = VectorSearchChain(\n",
    "    vectorstore=pgvector,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "query = \"Water leaking into the apartment from the floor above. What are the prominent legal precedents in Washington on this problem?\"\n",
    "results = vector_search_chain.run(query)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "\n",
    "# Set up OpenAI API (replace with your actual API key)\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = \n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a table for our documents\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        embedding vector(1536)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Function to get embeddings from OpenAI\n",
    "def get_embedding(text):\n",
    "    response = openai.embeddings.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "# Function to add a document\n",
    "def add_document(content):\n",
    "    embedding = get_embedding(content)\n",
    "    cur.execute(\"INSERT INTO documents (content, embedding) VALUES (%s, %s)\", (content, embedding))\n",
    "    conn.commit()\n",
    "\n",
    "# Function to search for similar documents\n",
    "def search_documents(query, limit=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT content, embedding <-> %s AS distance\n",
    "        FROM documents\n",
    "        ORDER BY distance\n",
    "        LIMIT %s\n",
    "    \"\"\", (query_embedding, limit))\n",
    "    return cur.fetchall()\n",
    "\n",
    "# Add some sample documents\n",
    "sample_docs = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Python is a high-level programming language.\",\n",
    "    \"Vector databases are essential for modern AI applications.\",\n",
    "    \"PostgreSQL is a powerful open-source relational database.\",\n",
    "]\n",
    "for doc in sample_docs:\n",
    "    add_document(doc)\n",
    "\n",
    "# Perform a search\n",
    "search_query = \"Tell me about programming languages\"\n",
    "results = search_documents(search_query)\n",
    "print(f\"Search results for: '{search_query}'\")\n",
    "for i, (content, distance) in enumerate(results, 1):\n",
    "    print(f\"{i}. {content} (Distance: {distance:.4f})\")\n",
    "\n",
    "# Clean up\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
